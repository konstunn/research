
\documentclass[a4paper,14pt]{extarticle}

% for ability to highlight\select and copy text from result pdf output to
% clipboard
\usepackage{cmap}

\usepackage{setspace}

\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}

\usepackage[a4paper,margin=1cm,footskip=1cm,left=2cm,right=1.5cm,top=1.5cm,
		bottom=2cm]{geometry}

\usepackage{textcase}
\usepackage{csquotes}
\usepackage{enumitem}

\usepackage[nottoc]{tocbibind}

% do not show section numbers
%\usepackage[raggedright]{titlesec}

\usepackage{amsmath}

% indent first paragraph in every section
\usepackage{indentfirst}

\usepackage{secdot}

\usepackage[titletoc,title]{appendix}


% indent description items
\setlist[description]{leftmargin=\parindent,labelindent=\parindent}

% ?
%\patchcmd{\appendices}{\quad}{: }{}{}

\setcounter{secnumdepth}{3}
\setstretch{1.5}

\DeclareMathOperator{\Sp}{Sp}

\begin{document}

\begin{titlepage}

	\begin{center}
		\MakeTextUppercase{Министерство образования и науки Российской~Федерации}

		\bigbreak

		ФЕДЕРАЛЬНОЕ ГОСУДАРСТВЕННОЕ БЮДЖЕТНОЕ ОБРАЗОВАТЕЛЬНОЕ УЧРЕЖДЕНИЕ
			ВЫСШЕГО ОБРАЗОВАНИЯ

		\bigbreak

		\MakeTextUppercase{\enquote{Новосибирский государственный технический
			университет}}
		\vspace{5pt}
		\hrule

		\bigbreak

		Кафедра теоретической и прикладной информатики

		\vspace{50pt}

		\textbf{\LARGE{Отчет по}\\}

		\bigbreak

		производственной практике: \\
			практике по получению профессиональных умений и опыта профессиональной
			деятельности

		\bigbreak

		c 1 ноября по 31 декабря

		\bigbreak

		<<Параметрическая идентификация непрерывно-дискретных стохастических
		динамических линейных систем>>

		\vspace{50pt}

	\end{center}

	\begin{flushleft}
		\begin{tabbing}
			Группа:\qquad\qquad\qquad \= ПММ-61\\
			Студент:                  \> Горбунов К. К.\\
			Место практики:           \> отдел № 8 ФГУП <<СНИИМ>> \\
			Руководитель:             \> нач. отдела № 8, Толстиков А.С.
		\end{tabbing}
	\end{flushleft}

	\begin{center}
		\vspace{\fill}
		Новосибирск, 2016 г.
	\end{center}

\end{titlepage}

\tableofcontents

\newpage

\section*{Введение}
\addcontentsline{toc}{section}{Введение}

В настоящее время математическое моделирование играет фундаментальную роль в
науке и технике и является одним из интенсивно развивающихся перспективных
научных направлений в области информатики.

Проблема идентификации, связанная с построением математических моделей по
экспериментальным данным, относится к одной из основных проблем теории и
практики автоматического управления. Ее качественное решение способствует
эффективному применению на практике современных математических методов и
наукоемких технологий, например, при расчете и проектировании систем управления
подвижными (в том числе авиационно-космическими) и технологическими объектами,
построении прогнозирующих моделей (например, в экономике и бизнес-процессах),
конструировании следящих и измерительных систем \cite{denisov}.

\section{Постановка задачи}

\subsection[Структурно-вероятностное описание модельной структуры]
{Структурно-вероятностное описание модельной \\структуры}

% remember to measure identification accuracy in spaces of outputs and
% parameters
%\noindent $\frac{||\theta^* -\ \hat{\theta}||}{||\theta^*||}$
%\\
%\\
%$\frac{||y^* -\ \hat{y}||}{||y^*||}$
%\\

Определим модель стохастической динамической линейной \\непрерывно-дискретной
системы в простанстве состояний в виде:
\begin{equation}
	\label{eq:initmod}
	\left\{ 
		\begin{array}{lll}
			\frac{d}{dt}x(t) &= F x(t) + C u(t) + G w(t), & t \in [t_0,T] \\ 
			y(t_k)           &= H x(t_k) + v(t_k),        & k = 1,\ldots, N
		\end{array} 
	\right. 
\end{equation}

Здесь:
\begin{description}
	\item [$x(t)$] -- вектор состояния;
	\item [$F$] -- матрица перехода состояния;
	\item [$u(t)$] -- вектор-функция управления (входного воздействия);
	\item [$C$] -- матрица управления;
	\item [$w(t)$] -- вектор возмущений;
	\item [$G$] -- матрица влияния возмущений;
	\item [$H$] -- матрица наблюдения;
	\item [$v(t_k)$] -- шум измерений;
	\item [$y(t_k)$] -- вектор наблюдений (измерений) отклика;
\end{description}

В данной модели уравнение объекта является непрерывным, а уравнение наблюдений
--- дискретным. Такая модель является характерной для значительного множества
прикладных задач.

Одной из целей исспедований в рамках магистерской диссертации является
разработка алгоритмов оценивания состояния и идентификации стохастической 
динамической непрерывно-дискретной модели вида \ref{eq:initmod}. 

Один из видимых путей решения задачи оценивания состояния такой системы
является адаптация аналогичных алгоритмов, применимых к дискретным и
непрерывным системам, к данному непрерывно-дискретному случаю.

Разрабатываемый алгоритм также должен оценивать вероятностные характеристики
шумов объекта и измерений, потому как в реальных задачах вероятностные
характеристики данных процессов неизвестны или же эти процессы являются
нестационарными.

\section[Параметрическая идентификация моделей стохастических
\\непрерывно-дискретных систем]
{Параметрическая идентификация моделей \\стохастических
непрерывно-дискретных систем} 

Идентификацией динамической системы называется определение структуры и
параметров математической модели, обеспечивающих наилучшее совпадение выходных
переменных моделей переменных модели и системы при одинаковых входных
воздействиях \cite{chubich}.

Под параметрической идентификацией, в частности, понимается только определение
параметров модели. Предполагается, что структура модели известна.

В данной работе в качестве критерия идентификации используется критерий
максимального правдоподобия, а получаемые оценки параметров модели является
оценками максимального правдоподобия (ОМП).

\subsection{Вычисление значения критерия идентификации}

\newcommand{\eps}{\varepsilon}

Выражение для критерия максимального правдоподобия имеет вид \cite{denisov}:
\begin{equation}
\begin{split}
\chi(\theta) = -\ln{L(Y_1^N;\theta)} = \frac{Nm}{2} \ln{2\pi} +
\\ + \frac{1}{2} \sum\limits_{k=1}^{N} 
\left[ \eps^T(t_k, \theta) B^{-1}(t_k, \theta) \eps(t_k, \theta) + 
\ln \det B(t_k, \theta) \right].
\end{split}
\end{equation}

Здесь:

\begin{description}
\item[$Y_1^N$] --- множество из $N$ наблюдений отклика
\item[$\theta$] --- вектор параметров модели 
\item[$\chi(\cdot)$] --- функционал, используемый в качестве критерия
идентификации
\item[$L(\cdot)$] --- функционал максимального правдоподобия 
\item[$N$] --- число дискретных моментов времени наблюдений
\item[$m$] --- число выходных каналов системы (размерность вектора наблюдений
отклика)
\item[$\eps(t_j) = y(t_j) - H \hat{x}(t_j|t_{j-1})$] --- обновляющая
последовательность 
\item[$B(t_j) = HP(t_j|t_{j-1})H^T + R$] --- ковариационная обновляющей
последовательности 
\end{description}

\newpage
Алгоритм вычисления значения критерия следующий:

\begin{enumerate}
\item Пусть $j = 1$, $\chi(\theta) = 0$.
\item Решаются дифференциальные уравнения непрерывно-дискретного фильтра
Калмана с соответствующими начальными условиями:
\[
\frac{d}{dt}\hat{x}(t|t_{j-1}) = F \hat{x}(t|t_{j-1}) + C u(t),\ 
t_{j-1} \le t \le t_j,\ \hat{x}(t_0|t_0) = \bar{x}_0;
\]
\[
\frac{d}{dt}P(t_j|t_{j-1}) = F P(t|t_{j-1}) + P(t|t_{j-1}) F^T + GQG^T,\\
\]
\[
t_{j-1} \le t \le t_j,\ P(t_0|t_0) = P_0;
\]
Получаем $\hat{x}(t_j|t_{j-1})$ и $P(t_j|t_{j-1})$.

\item Вычисляются
\[ \eps(t_j) = y(t_j) - H \hat{x}(t_j|t_{j-1}); \]
\[ B(t_j) = H P(t_j|t_{j-1}) H^T + R; \]
\[
K(t_j) = P(t_j|t_{j-1}) H^T B^{-1}(t_j).
\]

\item Вычисляется соответствующая $j$-му моменту времени составляющая функции
правдоподобия (2.35)
\[
S = \frac{1}{2} \left[ B^{-1}(t_j) \eps(t_j) + \ln \det B(t_j) \right].
\]

\item Накапливается сумма
\[
\chi(\theta) = \chi(\theta) + S.
\]

\item Если $j = N$, то
\[
\chi(\theta) = \chi(\theta) + \frac{Nm}{2} \ln 2\pi
\]
и вычисления прекращаются, иначе --- вычисляются следующие начальные условия по
формулам
\[
\hat{x}(t_j|t_{j-1}) = \hat{x}(t_j|t_{j-1}) + K(t_j) \eps(t_j);
\]
\[
P(t_j|t_{j-1}) = \left[ I - K(t_j) H \right] P(t_j|t_{j-1}),
\]
$j$ заменяется на $j+1$ и осуществляется переход на шаг 2.

\end{enumerate}

\subsection[Вычисление значения градиента критерия идентификации]
{Вычисление значения градиента критерия \\идентификации}

В данной работе предлагается к рассмотрению два практических метода вычисления
значения градиента критерия идентификации: по аналитическому выражению и с
использованием программных средств автоматического дифференцирования.

Далее приведено краткое описание каждого из этих методов.

\subsubsection{Вычисление по аналитическому выражению}


Аналитическое выражение производных критерия следующее \cite{denisov}:

\begin{multline}
	\label{eq:llderiv}
	\frac{\partial \chi(\theta)}{\partial \theta_k} = \sum\limits^{N}_{j=1}
	\left[
	\eps^{T}(t_j) B^{-1}(t_j) \frac{\partial \eps(t_j)}{\partial \theta_k} -
	\frac{1}{2} \eps^{T}(t_j) B^{-1}(t_j) \frac{\partial B(t_j)}{\partial \theta_k}
	B^{-1}(t_j) \eps(t_j) + \right.\\\left.
	\frac{1}{2}
	\Sp ( B^{-1}(t_j) \frac{\partial B(t_j)}{\partial \theta_k} )
	\right] ,\ k = 1,\ldots,s,
\end{multline}
где $s$ --- размерность вектора неизвестных параметров $\Theta$.

В выражении \ref{eq:llderiv} частные производные $\frac{\partial \eps(t_j)}
{\partial \theta_k}$ и $\frac{\partial B(t_j)}{\partial \theta_k}$ вычисляются
с помощью следующим выражений:
\begin{equation}
\frac{\partial \eps(t_j)}{\partial \theta_k} = - \frac{\partial H}
{\partial \theta_k} \hat{x}(t_j|t_{j-1}) - H \frac{\partial \hat{x}(t_j|t_{j-1})}
{\partial \theta_k},\\
\end{equation}

\begin{equation}
\frac{\partial B(t_j)}{\partial \theta_k} = \frac{\partial H}{\partial \theta_k}
P(t_j|t_{j-1}) H^T + H \frac{\partial P(t_j|t_{j-1})}{\partial \theta_k} H^T +
H P(t_j|t_{j-1}) \frac{\partial H^T}{\partial \theta_k} + \frac{\partial R}
{\partial \theta_k}.
\end{equation}

Уравнения чувствительности для оценки вектора состояний:
\begin{multline}
\frac{d}{dt}
\begin{bmatrix}
	\hat{x}(t|t_{j-1}) \\
	\frac{\partial \hat{x}(t|t_{j-1})}{\partial \theta_1} \\
	\vdots \\
	\frac{\partial \hat{x}(t|t_{j-1})}{\partial \theta_s}
\end{bmatrix} =
\begin{bmatrix}
	F \hat{x}(t|t_{j-1}) + G u(t) \\
	\frac{\partial F}{\partial \theta_1} \hat{x}(t|t_{j-1}) +
		F \frac{\partial \hat{x}(t|t_{j-1})}{\partial \theta_1} +
		\frac{\partial G}{\partial \theta_1} u(t) \\
	\vdots  \\
	\frac{\partial F}{\partial \theta_s} \hat{x}(t|t_{j-1}) +
		F \frac{\partial \hat{x}(t|t_{j-1})}{\partial \theta_s} +
		\frac{\partial G}{\partial \theta_s} u(t)
\end{bmatrix} = \\ =
\begin{bmatrix}
	F & 0 & \cdots & 0 \\
	\frac{\partial F}{\partial \theta_1} & F & \cdots & 0 \\
	\vdots & \vdots & \ddots & \vdots \\
	\frac{\partial F}{\partial \theta_s} & 0 & \cdots & F
\end{bmatrix} \cdot
\begin{bmatrix}
	\hat{x}(t|t_{j-1}) \\
	\frac{\partial \hat{x}(t|t_{j-1})}{\partial \theta_1} \\
	\vdots \\
	\frac{\partial \hat{x}(t|t_{j-1})}{\partial \theta_s} \\
\end{bmatrix} +
\begin{bmatrix}
	G \\
	\frac{\partial{G}}{\partial \theta_1} \\
	\vdots \\
	\frac{\partial{G}}{\partial \theta_s}
\end{bmatrix} u(t),\quad t_{j-1} \le t \le t_j,\ j=1,\ldots,N.
\end{multline}

Начальные условия:
\begin{itemize}

\item для $j = 1$

\begin{equation}
\begin{bmatrix}
	\hat{x}(t_0|t_0) \\
	\frac{\partial \hat{x}(t_0|t_0)}{\partial \theta_1} \\
	\vdots \\
	\frac{\partial \hat{x}(t_0|t_0)}{\partial \theta_s}
\end{bmatrix} =
\begin{bmatrix}
	\overline{x_0} \\
	\frac{\partial \overline{x_0}}{\partial \theta_1} \\
	\vdots \\
	\frac{\partial \overline{x_0}}{\partial \theta_s}
\end{bmatrix}
\end{equation}

\item для $j = 2, 3, \ldots, N$

\begin{equation}
\begin{bmatrix}
	\hat{x}(t_j|t_j) \\
	\frac{\partial \hat{x}(t_j|t_j)}{\partial \theta_1} \\
	\vdots \\
	\frac{\partial \hat{x}(t_j|t_j)}{\partial \theta_s}
\end{bmatrix} =
\begin{bmatrix}
	\hat{x}(t_j|t_{j-1}) + K(t_j) \eps(t_j) \\
	\frac{\partial \hat{x}(t_j|t_{j-1})}{\partial \theta_1} + \frac{\partial K(t_j)}
		{\partial \theta_1} \eps(t_j) + K(t_j) \frac{\partial \eps(t_j)}
		{\partial \theta_1} \\
		\vdots \\
	\frac{\partial \hat{x}(t_j|t_{j-1})}{\partial \theta_s} + \frac{\partial K(t_j)}
		{\partial \theta_s} \eps(t_j) + K(t_j) \frac{\partial \eps(t_j)}
		{\partial \theta_s}
\end{bmatrix}.
\end{equation}

\end{itemize}

Уравнения чувствительности для ковариационной матрицы оценки вектора состояний:
\begin{multline}
	\frac{d}{dt}
	\begin{bmatrix}
		P(t|t_{j-1}) \\
		\frac{\partial P(t|t_{j-1})}{\partial \theta_1} \\
		\vdots \\
		\frac{\partial P(t|t_{j-1})}{\partial \theta_s}
	\end{bmatrix} = \\
	= \begin{bmatrix}
		F P(t|t_{j-1}) + P(t|t_{j-1}) F^T + G Q G^T \\
		\frac{\partial F}{\partial \theta_1} P(t|t_{j-1}) +
			F \frac{\partial P(t|t_{j-1})}{\partial \theta_1} +
			\frac{\partial P(t|t_{j-1})}{\partial \theta_1} F^T + \hspace{\fill} \\
			\hspace{8em} + P(t|t_{j-1}) \frac{\partial F^T}{\partial \theta_1} +
			\frac{\partial G}{\partial \theta_1} Q G^T +
			G \frac{\partial Q}{\partial \theta_1} G^T +
			G Q \frac{\partial G^T}{\partial \theta_1} \\
		\vdots \\
		\frac{\partial F}{\partial \theta_s} P(t|t_{j-1}) +
			F \frac{\partial P(t|t_{j-1})}{\partial \theta_s} +
			\frac{\partial P(t|t_{j-1})}{\partial \theta_s} F^T + \hspace{\fill} \\
			\hspace{8em} + P(t|t_{j-1}) \frac{\partial F^T}{\partial \theta_s} +
			\frac{\partial G}{\partial \theta_s} Q G^T +
			G \frac{\partial Q}{\partial \theta_s} G^T +
			G Q \frac{\partial G^T}{\partial \theta_s}
	\end{bmatrix}  = \\
	= \begin{bmatrix}
		F & 0 & \cdots & 0 \\
		\frac{\partial F}{\partial \theta_1} & F & \cdots & 0 \\
		\vdots & \vdots & \ddots & \vdots \\
		\frac{\partial F}{\partial \theta_s} & 0 & \cdots & F \\
	\end{bmatrix}
	\begin{bmatrix}
		P(t|t_{j-1}) \\
		\frac{\partial P(t|t_{j-1})}{\partial \theta_1} \\
		\vdots \\
		\frac{\partial P(t|t_{j-1})}{\partial \theta_s}
	\end{bmatrix} +
	\begin{bmatrix}
	P(t|t_{j-1}) & 0 & \cdots & 0 \\
	\frac{\partial P(t|t_{j-1})}{\partial \theta_1} & P(t|t_{j-1}) & \cdots & 0 \\
	\vdots & \vdots & \ddots & \vdots \\
	\frac{\partial P(t|t_{j-1})}{\partial \theta_s} & 0 & \cdots & P(t|t_{j-1})
	\end{bmatrix}
	\begin{bmatrix}
		F^T \\
		\frac{\partial F^T}{\partial \theta_1} \\
		\vdots \\
		\frac{\partial F^T}{\partial \theta_s}
	\end{bmatrix} + \\
	+ \begin{bmatrix}
		G Q G^T\\
		\frac{\partial G}{\partial \theta_1} Q G^T \\
		\vdots \\
		\frac{\partial G}{\partial \theta_s} Q G^T
	\end{bmatrix} +
	\begin{bmatrix}
		0 \\
		G \frac{\partial Q}{\partial \theta_1} G^T \\
		\vdots \\
		G \frac{\partial Q}{\partial \theta_s} G^T
	\end{bmatrix} +
	\begin{bmatrix}
		0 \\
		G Q \frac{\partial G^T}{\partial \theta_1} \\
		\vdots \\
		G Q \frac{\partial G^T}{\partial \theta_s}
	\end{bmatrix},\\ \hspace{\fill} t_{j-1} \le t \le t_j, \quad j = 1, \ldots, N. \hspace{\fill}
\end{multline}

\newpage
Начальные условия:
\begin{itemize}

	\item для $j = 1$

\begin{equation}
\begin{bmatrix}
	P(t_0|t_0) \\
	\frac{\partial P(t_0|t_0)}{\partial \theta_1} \\
	\vdots \\
	\frac{\partial P(t_0|t_0)}{\partial \theta_s}
\end{bmatrix} =
\begin{bmatrix}
	P_0 \\
	\frac{\partial P_0}{\partial \theta_1} \\
	\vdots \\
	\frac{\partial P_0}{\partial \theta_s}
\end{bmatrix}
\end{equation}

\item для $j = 2, 3, \ldots, N$

\begin{multline}
\begin{bmatrix}
	P(t|t_j) \\
	\frac{\partial P(t_j|t_j)}{\partial \theta_1} \\
 	\vdots \\
	\frac{\partial P(t_j|t_j)}{\partial \theta_s}
\end{bmatrix} =
\begin{bmatrix}
	\left\{ I - K(t_j) H \right\} P(t_j|t_{j-1}) \\
	\left[ - \frac{\partial K(t_j)}{\partial \theta_1} H - K(t_j)
		\frac{\partial H}{\partial \theta_1} \right] P(t_j|t_{j-1}) + 
		\left\{ I - K(t_j) H \right\}
		\frac{\partial P(t_j|t_{j-1})}{\partial \theta_1} \\
	\vdots \\
	\left[ - \frac{\partial K(t_j)}{\partial \theta_s} H - K(t_j)
		\frac{\partial H}{\partial \theta_s} \right] P(t_j|t_{j-1}) + 
		\left\{ I - K(t_j) H \right\}
		\frac{\partial P(t_j|t_{j-1})}{\partial \theta_s} \\
\end{bmatrix},
\end{multline}

\end{itemize}

где
\begin{multline}
	\frac{\partial K(t_j)}{\partial \theta_k} =
		\frac{\partial P(t_j|t_{j-1})}{\partial \theta_k} H^T B^{-1}(t_j) + 
		P(t_j|t_{j-1}) \frac{\partial H^T}{\partial \theta_k} B^{-1}(t_j) - \\ -
		P(t_j|t{j-1}) H^{-1} B^{-1}(t_j) \frac{\partial B(t_j)}{\partial \theta_k}
		B^{-1}(t_j).
\end{multline}

Алгоритм может быть следующим:

\begin{enumerate}
\item первый пунк
\end{enumerate}

\subsubsection{Программный метод автоматического дифференцирования}

Рассматриваемый программный метод берет свое начало из задач машинного обучения
глубоких нейронных сетей, которые в настоящее время являются популярным
объектом научных исследований, образующие целое новое научное направление ---
глубинное обучение. Глубокие нейронные сети применяются для решения широкого
класса практических задач распознавания, классификации, предсказания, обработки
и анализа ествесственных языков и многих других.

Глубокими нейронными сетями называются такие сети, которые имеют в своем
составе большое число скрытых слоев нейронов, и, следовательно, суммарно
большое число нейронов.

Задачу обучения нейронной сети можно трактовать как оценивание параметров этой
сети по некоторым полученным извне наборам данных (наблюдениям), которые
называются обучающими наборами данными. Параметрами сети являются веса связей
между её нейронами. Так как по определению глубокой сети число входящих в неё
нейронов велико, то, соответственно, велико и число <<оцениваемых>> параметров
(весов).

В связи с большим числом параметров (весов) возникают проблемы применимости
методов оценивания с использованием традиционных методов вычисления градиента
функции потерь (критерия) для поиска экстремальных значений. Эти проблемы,
связаны, во-первых, с невозможностью вывода аналитического выражения градиента
для модели со столь большим числом параметров и столь сложной структурой как
нейронная сеть. Во-вторых, методы приближенного вычисления градиента не
позволяют достичь удовлетворительной точности для обеспечения сходимости
оптимизационной процедуры к искомому решению.

Программный метод автоматического дифференцирования или также называемый метод
обратного распространения ошибки основан на правиле дифференцирования сложной
функции --- <<правиле цепи>> (<<chain rule>>), и предполагает определения
производной для каждой операции используемой системы компьютерной алгебры.
Данный метод позволяет вычислять градиент быстро и точно, не программируя явно
его вычисления. Отсюда и название метода --- автоматическое дифференцирование.
Следует отметить, очевидно, что с помощью этого метода также можно вычислять
и производные высших порядков быстро и без потери точности.

% вставить рисунки

Наиболее популярные системы, в которых реализовано автоматическое
дифференцирование: TensorFlow, Theano, Torch. Существует большое число других
систем и средств. В данной работе используется система TensorFlow.

\subsection{Программные аспекты реализации вычислений}

В данной работе используется библиотека для глубинного машинного обучения и
система компьютерной алгебры TensorFlow. В этой системе всякая вычислительная
программа является ориентированным (вычислительным) графом, в котором вершины
являются операциями или данными, а ребра указывают направления <<движения>>
данных. Основной структурой данных является Tensor --- обобщение скаляров,
векторов-столбцов, строк, квадратных и прямоугольных матриц и вообще любого
многомерного массива. Отсюда и название системы <<TensorFlow>> --- <<поток
тензоров>>.

Программу, использующую TensorFlow, в простейшем случае можно условно разделить
на две части: в первой части определяется вычислительный граф, во второй части
этот граф запускается, исполняется. Создание графа --- процесс относительно
медленный и занимает единицы, а может и десятки секунд, этот процесс можно
сравнить с компиляцией. Зато по успешному завершению созданию графа получаем
быструю, эффективную, параллельную вычислительную программу, которая в
подавляющем большинстве случаев будет превосходить по скорости программы,
написанные за некоторое короткое время с нуля на языке C/С++.

Система TensorFlow использует результаты теории графов и позволяет эффективно 
распараллеливать алгоритмы. К тому же в системе заложен функционал, позволяющий
производить вычисления распределенно, на нескольких машинах.

\section[Результаты экспериментальных модельных исследований]
{Результаты экспериментальных модельных \\исследований} 

Результаты.

\section*{Заключение}
\addcontentsline{toc}{section}{Заключение}

Заключение.

\begin{thebibliography}{9}

\begin{hyphenrules}{nohyphenation} 

\begin{sloppypar}

\bibitem{denisov} Активная параметрическая идентификация стохастических
линейных систем: монография / В.И. Денисов, В.М. Чубич, О.С. Черникова, Д.И.
Бобылева. --- Новосибирск : Изд-во НГТУ, 2009. --- 192 с. (Серия <<Монографии
НГТУ>>).

\bibitem{chubich} Активная параметрическая идентификация стохастических
динамических систем. Оценивание параметров: учеб. пособие / В.М. Чубич, Е.В.
Филиппова. --- Новосибирск: Изд-во НГТУ, 2016. --- 63 с.

\bibitem{shalom} Bar-Shalom, Yaakov. Estimation with Application to Tracking
and Navigation / by Yaakov Bar-Shalom, X.-Rong Li, Thiagalingam Kirubarajan. 
	- 2001.

\bibitem{ogarkov} Огарков М. А. Методы статистического оценивания параметров
случайных процессов. --- М.: Энергоатомиздат, 1990. --- 208 с.: ил.

\end{sloppypar}

\end{hyphenrules}

\end{thebibliography}

\begin{appendices}

\section{Исходные тексты программ}

\subsection{Вычисление критерия идентификации}

\subsection{Вычисление градиента критерия идентификации}

\end{appendices}

\end{document}

# vim: ts=2 sw=2
